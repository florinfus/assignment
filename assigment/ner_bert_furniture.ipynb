{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Job\\Assignment Soleadify\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Also available in a selection of other colors...</td>\n",
       "      <td>[O, O, O, I-LOC, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ULUDAGE SOFA SET 3M+3+1, $('.swatch[data-opti...</td>\n",
       "      <td>[I-LOC, O, O, O, I-LOC, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Dhs. 2,500.00, With an innovative blend of pl...</td>\n",
       "      <td>[O, O, O, I-LOC, O, O, O, I-LOC, O, O, O, O, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0510, Love to host but short on space? The la...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, I-LOC,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0419, $('.swatch[data-option-index=\"0\"] .aed-...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, I-LOC, O, O, O, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2629</th>\n",
       "      <td>[48\" Round Outdoor Teak Dining Table Handcraft...</td>\n",
       "      <td>[O, O, O, I-LOC, O, I-LOC, O, O, O, O, I-LOC, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2630</th>\n",
       "      <td>[7 pc Venice Teak Deep Seating Deluxe Sofa wit...</td>\n",
       "      <td>[I-LOC, I-LOC, I-LOC, O, I-LOC, I-LOC, O, I-LO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631</th>\n",
       "      <td>[Email Customer Service, Daybed 72\"Lx63.5\"Wx16...</td>\n",
       "      <td>[O, O, I-LOC, O, O, O, O, I-LOC, O, O, I-LOC, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2632</th>\n",
       "      <td>[$8,194.99, $4,289.99, Sectional Sofa 119\"x32....</td>\n",
       "      <td>[O, O, O, I-LOC, O, I-LOC, O, O, I-LOC, O, I-L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633</th>\n",
       "      <td>[9pc pc Hermosa Teak Sectional with 72\" Coffee...</td>\n",
       "      <td>[I-LOC, O, O, O, I-LOC, O, O, O, O, O, O, I-LO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2634 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tokens  \\\n",
       "0     [Also available in a selection of other colors...   \n",
       "1     [ULUDAGE SOFA SET 3M+3+1, $('.swatch[data-opti...   \n",
       "2     [Dhs. 2,500.00, With an innovative blend of pl...   \n",
       "3     [0510, Love to host but short on space? The la...   \n",
       "4     [0419, $('.swatch[data-option-index=\"0\"] .aed-...   \n",
       "...                                                 ...   \n",
       "2629  [48\" Round Outdoor Teak Dining Table Handcraft...   \n",
       "2630  [7 pc Venice Teak Deep Seating Deluxe Sofa wit...   \n",
       "2631  [Email Customer Service, Daybed 72\"Lx63.5\"Wx16...   \n",
       "2632  [$8,194.99, $4,289.99, Sectional Sofa 119\"x32....   \n",
       "2633  [9pc pc Hermosa Teak Sectional with 72\" Coffee...   \n",
       "\n",
       "                                               ner_tags  \n",
       "0     [O, O, O, I-LOC, O, O, O, O, O, O, O, O, O, O,...  \n",
       "1     [I-LOC, O, O, O, I-LOC, O, O, O, O, O, O, O, O...  \n",
       "2     [O, O, O, I-LOC, O, O, O, I-LOC, O, O, O, O, I...  \n",
       "3     [O, O, O, O, O, O, O, O, O, O, O, O, O, I-LOC,...  \n",
       "4     [O, O, O, O, O, O, O, O, O, I-LOC, O, O, O, I-...  \n",
       "...                                                 ...  \n",
       "2629  [O, O, O, I-LOC, O, I-LOC, O, O, O, O, I-LOC, ...  \n",
       "2630  [I-LOC, I-LOC, I-LOC, O, I-LOC, I-LOC, O, I-LO...  \n",
       "2631  [O, O, I-LOC, O, O, O, O, I-LOC, O, O, I-LOC, ...  \n",
       "2632  [O, O, O, I-LOC, O, I-LOC, O, O, I-LOC, O, I-L...  \n",
       "2633  [I-LOC, O, O, O, I-LOC, O, O, O, O, O, O, I-LO...  \n",
       "\n",
       "[2634 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_file = 'furniture/furniture_train_df.gzip'\n",
    "train_df = pd.read_parquet(train_df_file)\n",
    "train_df.update(train_df.tokens.apply(np.ndarray.tolist))\n",
    "train_df.update(train_df.ner_tags.apply(np.ndarray.tolist))\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Welded steel rods with polished chrome finish...</td>\n",
       "      <td>[O, O, O, O, O, O, O, I-LOC, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Our Touch family of designs emphasizes the be...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Tall / Walnut / Slate, Losanges I, $5,257.00,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[View Spot Stool Bar Height, 18 - 27.16\" diam ...</td>\n",
       "      <td>[O, O, O, O, O, O, I-LOC, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Smooth tilt mimics body's natural pivot point...</td>\n",
       "      <td>[O, O, O, O, O, O, I-LOC, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>[Jada Stool by , Sorin Bed Pristine Vintage Fa...</td>\n",
       "      <td>[I-LOC, I-LOC, I-LOC, O, I-LOC, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>[Marlowe Stool Fur by , $1,634.00, This produc...</td>\n",
       "      <td>[I-LOC, O, O, O, I-LOC, O, O, I-LOC, I-LOC, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>[$720.00, Fergie Stool Cerused Oak by , Venus ...</td>\n",
       "      <td>[O, I-LOC, O, O, O, O, I-LOC, O, O, I-LOC, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>[Zanzibar Side Table and Stool Driftwood by , ...</td>\n",
       "      <td>[I-LOC, I-LOC, I-LOC, O, O, I-LOC, I-LOC, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>[Lucy Bistro Table Carbon Wash, $2,950.00, Har...</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>968 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tokens  \\\n",
       "0    [Welded steel rods with polished chrome finish...   \n",
       "1    [Our Touch family of designs emphasizes the be...   \n",
       "2    [Tall / Walnut / Slate, Losanges I, $5,257.00,...   \n",
       "3    [View Spot Stool Bar Height, 18 - 27.16\" diam ...   \n",
       "4    [Smooth tilt mimics body's natural pivot point...   \n",
       "..                                                 ...   \n",
       "963  [Jada Stool by , Sorin Bed Pristine Vintage Fa...   \n",
       "964  [Marlowe Stool Fur by , $1,634.00, This produc...   \n",
       "965  [$720.00, Fergie Stool Cerused Oak by , Venus ...   \n",
       "966  [Zanzibar Side Table and Stool Driftwood by , ...   \n",
       "967  [Lucy Bistro Table Carbon Wash, $2,950.00, Har...   \n",
       "\n",
       "                                              ner_tags  \n",
       "0    [O, O, O, O, O, O, O, I-LOC, O, O, O, O, O, O,...  \n",
       "1     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "2    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3    [O, O, O, O, O, O, I-LOC, O, O, O, O, O, O, O,...  \n",
       "4    [O, O, O, O, O, O, I-LOC, O, O, O, O, O, O, O,...  \n",
       "..                                                 ...  \n",
       "963  [I-LOC, I-LOC, I-LOC, O, I-LOC, O, O, O, O, O,...  \n",
       "964  [I-LOC, O, O, O, I-LOC, O, O, I-LOC, I-LOC, I-...  \n",
       "965  [O, I-LOC, O, O, O, O, I-LOC, O, O, I-LOC, O, ...  \n",
       "966  [I-LOC, I-LOC, I-LOC, O, O, I-LOC, I-LOC, O, O...  \n",
       "967                                          [O, O, O]  \n",
       "\n",
       "[968 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_file = 'furniture/furniture_test_df.gzip'\n",
    "test_df = pd.read_parquet(test_df_file)\n",
    "test_df.update(test_df.tokens.apply(np.ndarray.tolist))\n",
    "test_df.update(test_df.ner_tags.apply(np.ndarray.tolist))\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Fus/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\Fus/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\Fus/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Fus/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\Fus/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_list = ['O', 'I-LOC']\n",
    "label_encoding_dict = {'O':0, 'I-LOC':1}\n",
    "\n",
    "task = \"ner\" \n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    label_all_tokens = True\n",
    "    tokenized_inputs = tokenizer(list(examples[\"tokens\"]), truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif label[word_idx] == '0':\n",
    "                label_ids.append(0)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_encoding_dict[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(label_encoding_dict[label[word_idx]] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "        \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.04ba/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokenized_datasets = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_tokenized_datasets = test_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Also available in a selection of other colors...</td>\n",
       "      <td>[O, O, O, I-LOC, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[101, 2036, 2800, 1999, 1037, 4989, 1997, 2060...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ULUDAGE SOFA SET 3M+3+1, $('.swatch[data-opti...</td>\n",
       "      <td>[I-LOC, O, O, O, I-LOC, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[101, 17359, 14066, 3351, 10682, 2275, 1017, 2...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Dhs. 2,500.00, With an innovative blend of pl...</td>\n",
       "      <td>[O, O, O, I-LOC, O, O, O, I-LOC, O, O, O, O, I...</td>\n",
       "      <td>[101, 28144, 2015, 1012, 1016, 1010, 3156, 101...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0510, Love to host but short on space? The la...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, I-LOC,...</td>\n",
       "      <td>[101, 5709, 10790, 2293, 2000, 3677, 2021, 246...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0419, $('.swatch[data-option-index=\"0\"] .aed-...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, I-LOC, O, O, O, I-...</td>\n",
       "      <td>[101, 5840, 16147, 1002, 1006, 1005, 1012, 254...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2629</th>\n",
       "      <td>[48\" Round Outdoor Teak Dining Table Handcraft...</td>\n",
       "      <td>[O, O, O, I-LOC, O, I-LOC, O, O, O, O, I-LOC, ...</td>\n",
       "      <td>[101, 4466, 1000, 2461, 7254, 5572, 2243, 7759...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2630</th>\n",
       "      <td>[7 pc Venice Teak Deep Seating Deluxe Sofa wit...</td>\n",
       "      <td>[I-LOC, I-LOC, I-LOC, O, I-LOC, I-LOC, O, I-LO...</td>\n",
       "      <td>[101, 1021, 7473, 7914, 5572, 2243, 2784, 1074...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631</th>\n",
       "      <td>[Email Customer Service, Daybed 72\"Lx63.5\"Wx16...</td>\n",
       "      <td>[O, O, I-LOC, O, O, O, O, I-LOC, O, O, I-LOC, ...</td>\n",
       "      <td>[101, 10373, 8013, 2326, 2154, 8270, 5824, 100...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2632</th>\n",
       "      <td>[$8,194.99, $4,289.99, Sectional Sofa 119\"x32....</td>\n",
       "      <td>[O, O, O, I-LOC, O, I-LOC, O, O, I-LOC, O, I-L...</td>\n",
       "      <td>[101, 1002, 1022, 1010, 19955, 1012, 5585, 100...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633</th>\n",
       "      <td>[9pc pc Hermosa Teak Sectional with 72\" Coffee...</td>\n",
       "      <td>[I-LOC, O, O, O, I-LOC, O, O, O, O, O, O, I-LO...</td>\n",
       "      <td>[101, 1023, 15042, 7473, 2014, 15530, 2050, 55...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2634 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tokens  \\\n",
       "0     [Also available in a selection of other colors...   \n",
       "1     [ULUDAGE SOFA SET 3M+3+1, $('.swatch[data-opti...   \n",
       "2     [Dhs. 2,500.00, With an innovative blend of pl...   \n",
       "3     [0510, Love to host but short on space? The la...   \n",
       "4     [0419, $('.swatch[data-option-index=\"0\"] .aed-...   \n",
       "...                                                 ...   \n",
       "2629  [48\" Round Outdoor Teak Dining Table Handcraft...   \n",
       "2630  [7 pc Venice Teak Deep Seating Deluxe Sofa wit...   \n",
       "2631  [Email Customer Service, Daybed 72\"Lx63.5\"Wx16...   \n",
       "2632  [$8,194.99, $4,289.99, Sectional Sofa 119\"x32....   \n",
       "2633  [9pc pc Hermosa Teak Sectional with 72\" Coffee...   \n",
       "\n",
       "                                               ner_tags  \\\n",
       "0     [O, O, O, I-LOC, O, O, O, O, O, O, O, O, O, O,...   \n",
       "1     [I-LOC, O, O, O, I-LOC, O, O, O, O, O, O, O, O...   \n",
       "2     [O, O, O, I-LOC, O, O, O, I-LOC, O, O, O, O, I...   \n",
       "3     [O, O, O, O, O, O, O, O, O, O, O, O, O, I-LOC,...   \n",
       "4     [O, O, O, O, O, O, O, O, O, I-LOC, O, O, O, I-...   \n",
       "...                                                 ...   \n",
       "2629  [O, O, O, I-LOC, O, I-LOC, O, O, O, O, I-LOC, ...   \n",
       "2630  [I-LOC, I-LOC, I-LOC, O, I-LOC, I-LOC, O, I-LO...   \n",
       "2631  [O, O, I-LOC, O, O, O, O, I-LOC, O, O, I-LOC, ...   \n",
       "2632  [O, O, O, I-LOC, O, I-LOC, O, O, I-LOC, O, I-L...   \n",
       "2633  [I-LOC, O, O, O, I-LOC, O, O, O, O, O, O, I-LO...   \n",
       "\n",
       "                                              input_ids  \\\n",
       "0     [101, 2036, 2800, 1999, 1037, 4989, 1997, 2060...   \n",
       "1     [101, 17359, 14066, 3351, 10682, 2275, 1017, 2...   \n",
       "2     [101, 28144, 2015, 1012, 1016, 1010, 3156, 101...   \n",
       "3     [101, 5709, 10790, 2293, 2000, 3677, 2021, 246...   \n",
       "4     [101, 5840, 16147, 1002, 1006, 1005, 1012, 254...   \n",
       "...                                                 ...   \n",
       "2629  [101, 4466, 1000, 2461, 7254, 5572, 2243, 7759...   \n",
       "2630  [101, 1021, 7473, 7914, 5572, 2243, 2784, 1074...   \n",
       "2631  [101, 10373, 8013, 2326, 2154, 8270, 5824, 100...   \n",
       "2632  [101, 1002, 1022, 1010, 19955, 1012, 5585, 100...   \n",
       "2633  [101, 1023, 15042, 7473, 2014, 15530, 2050, 55...   \n",
       "\n",
       "                                         attention_mask  \\\n",
       "0     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                 ...   \n",
       "2629  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2630  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2631  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2632  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2633  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                 labels  \n",
       "0     [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1     [-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...  \n",
       "2     [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3     [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4     [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "2629  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2630  [-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2631  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2632  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2633  [-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "\n",
       "[2634 rows x 5 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized_datasets_df = pd.DataFrame(train_tokenized_datasets)\n",
    "train_tokenized_datasets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Also available in a selection of other colors',\n",
       " 'Dhs. 2,240.00',\n",
       " '$(\\'.swatch[data-option-index=\"0\"] .brown-tosc-06-swatch\\', \\'#product-form-1823748587620product-templa',\n",
       " 'ALBERO TV UNIT (FULL SET)',\n",
       " '$(\\'.swatch[data-option-index=\"0\"] .60-cm-swatch\\', \\'#product-form-2549553660004product-template\\').rem',\n",
       " 'Dhs. 1,785.00',\n",
       " '0171',\n",
       " '850 grms Hard Felts (country of origin Turkey) on both Sides.',\n",
       " '0789',\n",
       " '120 X 200 CM',\n",
       " 'Dhs. 18,760.00',\n",
       " 'Dhs. 11,999.00',\n",
       " 'MDF ending at the back.',\n",
       " 'Offer your dining room a definitively traditional style with this spindle-back chair. Creating a ref',\n",
       " 'WOODEN',\n",
       " 'Dimensions : H 108 x W 53 x D 45 cm.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized_datasets_df['tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized_datasets_df['ner_tags'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2036,\n",
       " 2800,\n",
       " 1999,\n",
       " 1037,\n",
       " 4989,\n",
       " 1997,\n",
       " 2060,\n",
       " 6087,\n",
       " 28144,\n",
       " 2015,\n",
       " 1012,\n",
       " 1016,\n",
       " 1010,\n",
       " 11212,\n",
       " 1012,\n",
       " 4002,\n",
       " 1002,\n",
       " 1006,\n",
       " 1005,\n",
       " 1012,\n",
       " 25414,\n",
       " 2818,\n",
       " 1031,\n",
       " 2951,\n",
       " 1011,\n",
       " 5724,\n",
       " 1011,\n",
       " 5950,\n",
       " 1027,\n",
       " 1000,\n",
       " 1014,\n",
       " 1000,\n",
       " 1033,\n",
       " 1012,\n",
       " 2829,\n",
       " 1011,\n",
       " 2000,\n",
       " 11020,\n",
       " 1011,\n",
       " 5757,\n",
       " 1011,\n",
       " 25414,\n",
       " 2818,\n",
       " 1005,\n",
       " 1010,\n",
       " 1005,\n",
       " 1001,\n",
       " 4031,\n",
       " 1011,\n",
       " 2433,\n",
       " 1011,\n",
       " 12522,\n",
       " 2581,\n",
       " 18139,\n",
       " 27814,\n",
       " 2581,\n",
       " 2575,\n",
       " 11387,\n",
       " 21572,\n",
       " 8566,\n",
       " 6593,\n",
       " 1011,\n",
       " 8915,\n",
       " 8737,\n",
       " 2721,\n",
       " 2632,\n",
       " 5677,\n",
       " 2080,\n",
       " 2694,\n",
       " 3131,\n",
       " 1006,\n",
       " 2440,\n",
       " 2275,\n",
       " 1007,\n",
       " 1002,\n",
       " 1006,\n",
       " 1005,\n",
       " 1012,\n",
       " 25414,\n",
       " 2818,\n",
       " 1031,\n",
       " 2951,\n",
       " 1011,\n",
       " 5724,\n",
       " 1011,\n",
       " 5950,\n",
       " 1027,\n",
       " 1000,\n",
       " 1014,\n",
       " 1000,\n",
       " 1033,\n",
       " 1012,\n",
       " 3438,\n",
       " 1011,\n",
       " 4642,\n",
       " 1011,\n",
       " 25414,\n",
       " 2818,\n",
       " 1005,\n",
       " 1010,\n",
       " 1005,\n",
       " 1001,\n",
       " 4031,\n",
       " 1011,\n",
       " 2433,\n",
       " 1011,\n",
       " 22234,\n",
       " 2683,\n",
       " 24087,\n",
       " 21619,\n",
       " 16086,\n",
       " 8889,\n",
       " 2549,\n",
       " 21572,\n",
       " 8566,\n",
       " 6593,\n",
       " 1011,\n",
       " 23561,\n",
       " 1005,\n",
       " 1007,\n",
       " 1012,\n",
       " 2128,\n",
       " 2213,\n",
       " 28144,\n",
       " 2015,\n",
       " 1012,\n",
       " 1015,\n",
       " 1010,\n",
       " 6275,\n",
       " 2629,\n",
       " 1012,\n",
       " 4002,\n",
       " 5890,\n",
       " 2581,\n",
       " 2487,\n",
       " 15678,\n",
       " 24665,\n",
       " 5244,\n",
       " 2524,\n",
       " 2371,\n",
       " 2015,\n",
       " 1006,\n",
       " 2406,\n",
       " 1997,\n",
       " 4761,\n",
       " 4977,\n",
       " 1007,\n",
       " 2006,\n",
       " 2119,\n",
       " 3903,\n",
       " 1012,\n",
       " 5718,\n",
       " 2620,\n",
       " 2683,\n",
       " 6036,\n",
       " 1060,\n",
       " 3263,\n",
       " 4642,\n",
       " 28144,\n",
       " 2015,\n",
       " 1012,\n",
       " 2324,\n",
       " 1010,\n",
       " 24643,\n",
       " 1012,\n",
       " 4002,\n",
       " 28144,\n",
       " 2015,\n",
       " 1012,\n",
       " 2340,\n",
       " 1010,\n",
       " 25897,\n",
       " 1012,\n",
       " 4002,\n",
       " 9108,\n",
       " 2546,\n",
       " 4566,\n",
       " 2012,\n",
       " 1996,\n",
       " 2067,\n",
       " 1012,\n",
       " 3749,\n",
       " 2115,\n",
       " 7759,\n",
       " 2282,\n",
       " 1037,\n",
       " 15764,\n",
       " 2135,\n",
       " 3151,\n",
       " 2806,\n",
       " 2007,\n",
       " 2023,\n",
       " 6714,\n",
       " 10362,\n",
       " 1011,\n",
       " 2067,\n",
       " 3242,\n",
       " 1012,\n",
       " 4526,\n",
       " 1037,\n",
       " 25416,\n",
       " 4799,\n",
       " 9646,\n",
       " 1024,\n",
       " 1044,\n",
       " 10715,\n",
       " 1060,\n",
       " 1059,\n",
       " 5187,\n",
       " 1060,\n",
       " 1040,\n",
       " 3429,\n",
       " 4642,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized_datasets_df['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Fus/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Fus/.cache\\huggingface\\hub\\models--distilbert-base-uncased\\snapshots\\1c4513b2eedbda136f57676a34eea67aba266e5c\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"test-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=1e-5,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] \n",
    "                        for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] \n",
    "                   for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "d:\\Job\\Assignment Soleadify\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2634\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 495\n",
      "  Number of trainable parameters = 66364418\n",
      "  0%|          | 0/495 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 33%|███▎      | 165/495 [31:26<57:41, 10.49s/it]  The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 968\n",
      "  Batch size = 16\n",
      "                                                 \n",
      " 33%|███▎      | 165/495 [35:24<57:41, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31776630878448486, 'eval_precision': 0.013709677419354839, 'eval_recall': 0.028317601332593003, 'eval_f1': 0.01847491396486144, 'eval_accuracy': 0.8613398225697849, 'eval_runtime': 237.64, 'eval_samples_per_second': 4.073, 'eval_steps_per_second': 0.257, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 330/495 [1:07:05<27:56, 10.16s/it]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 968\n",
      "  Batch size = 16\n",
      "                                                   \n",
      " 67%|██████▋   | 330/495 [1:11:03<27:56, 10.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3534741699695587, 'eval_precision': 0.024605214836577304, 'eval_recall': 0.03720155469183787, 'eval_f1': 0.02961980548187445, 'eval_accuracy': 0.8881197358628755, 'eval_runtime': 238.194, 'eval_samples_per_second': 4.064, 'eval_steps_per_second': 0.256, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [1:42:38<00:00, 11.11s/it]  The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 968\n",
      "  Batch size = 16\n",
      "                                                   \n",
      "100%|██████████| 495/495 [1:46:36<00:00, 11.11s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 495/495 [1:46:36<00:00, 12.92s/it]\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens. If ner_tags, tokens are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 968\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4284403324127197, 'eval_precision': 0.02480270574971815, 'eval_recall': 0.036646307606885066, 'eval_f1': 0.029583146571044373, 'eval_accuracy': 0.8862769187152257, 'eval_runtime': 237.944, 'eval_samples_per_second': 4.068, 'eval_steps_per_second': 0.256, 'epoch': 3.0}\n",
      "{'train_runtime': 6396.4995, 'train_samples_per_second': 1.235, 'train_steps_per_second': 0.077, 'train_loss': 0.04418554980345447, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [03:53<00:00,  3.83s/it]\n",
      "Saving model checkpoint to furniture-ner2.model\n",
      "Configuration saved in furniture-ner2.model\\config.json\n",
      "Model weights saved in furniture-ner2.model\\pytorch_model.bin\n",
      "tokenizer config file saved in furniture-ner2.model\\tokenizer_config.json\n",
      "Special tokens file saved in furniture-ner2.model\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model,\n",
    "                  args,\n",
    "                  train_dataset=train_tokenized_datasets,\n",
    "                  eval_dataset=test_tokenized_datasets,\n",
    "                  data_collator=data_collator,\n",
    "                  tokenizer=tokenizer,\n",
    "                  compute_metrics=compute_metrics)\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.save_model('furniture-ner2.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./furniture-ner.model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2077, 18207, 2582, 1010, 1045, 2323, 2066, 2000, 12367, 2372, 2008, 2895, 2006, 4433, 5813, 4921, 1010, 4709, 3663, 1997, 2529, 2916, 1997, 20996, 12053, 3148, 7486, 1998, 2060, 14302, 1999, 12620, 2003, 14475, 2000, 1037, 2101, 3058, 2000, 3499, 2051, 2005, 1996, 3319, 1997, 2049, 4746, 5166, 13494, 2011, 1996, 3587, 2837, 1012, 1996, 3320, 2097, 2202, 2895, 2006, 4433, 5813, 4921, 2004, 2574, 2004, 1996, 3189, 1997, 1996, 3587, 2837, 2006, 1996, 4746, 5166, 13494, 2003, 2800, 1012, 1045, 2085, 2507, 1996, 2723, 2000, 10656, 2015, 10261, 2000, 8116, 17959, 1997, 3789, 2030, 2597, 2077, 6830, 2030, 9886, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = '''Before proceeding further, I should like to inform members that action on draft resolution iv, entitled situation of human rights of Rohingya Muslims and other minorities in Myanmar is postponed to a later date to allow time for the review of its programme budget implications by the fifth committee. The assembly will take action on draft resolution iv as soon as the report of the fifth committee on the programme budget implications is available. I now give the floor to delegations wishing to deliver explanations of vote or position before voting or adoption.'''\n",
    "tokens_un = tokenizer(paragraph)\n",
    "tokens_un"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Candles, Candles, - Desk, - Table, Champagne, - Sideboard, Fragrance Family:, - Standing, A handblown glass vessel crafted to enhance the burning experience., TemaHome, - Innovation Living, 0, - Lounge Chair, Bench, - Side Table, Soho Concept, Rhubarb, Essential Oil Based Fragrances, Greenington, Accessories, Your cart, Coffee Table, 0 Items, Mirrors, LAMPS, Phone : 347-457-5727, DINING, Innovative, Top notes are the first impression of a fragrance., Ottoman, 100% Cotton Wick, Middle notes are the heart the of fragrance., Sectional Sofa, - Eilersen, Bottom notes are the final and lasting impression., Lounge Chair, Amber, - Bench, Four Hands, - Ottoman, BRANDS, - Varier, Cassis Blush, WORKING, - Hanging, MIDDLE:, Pin It, Hanging, Fatboy, More from this collection, Orange Blossom, Facebook, Instagram, - Greenington, Login, Table Lamps, Fragrance Life:, Weight:, Create account |, - Stool, - Sectional Sofa, - Chair, Storage, 820 Manhattan Ave, Brooklyn, NY 11222, ABOUT US, Your cart (0), - Sofa Bed, - Coffee Table, Tweet, Fragrance Notes:, SALE, 4\" D x 4\" H, Up to a 90 hour burn time., - Innovative, Grapefruit, Sideboard, Night Table, Hours : Mon-Sat 11am-7pm, Sun 12-5pm, TOP:, |, Varier, - Office Chair, - Four Hands, Stool, Table, Nuevo, Musk, $ 70.00, - Table Lamps, Chair, Vetiver, Side Table, Eilersen, POLICY, Pinterest, Mandarin Leaves, Sofa Bed, Innovation Living, Green Fern, 15.5oz, Dimensions:, - Sofa, - Nuevo, Bookcase, Enlarge Image, - Fatboy, Key Ingredients in Formula:, The vessel can be reused as a part of your home decor., SLEEPING, Sofa, Office Chair, White Grapefruit, Bed, The parted cabana curtains reveal a plate of freshly-picked fruit hidden from the summer sun. A mouthwatering blend of pineapple and grapefruit nestles on a bed of green fern and mandarin leaves. The irresistible scent is warmed by sumptuous notes of apple wood and orange blossom., Citrus, Desk, Soy Wax, - Gus Modern, Pineapple, - Night Table, Orange Brazil, - Bookcase, - TemaHome, Standing, - Mirrors, Details, Chamomile Lavender, LIVING, - Storage, Applewood, BOTTOM:, Feu de Bois, - Soho Concept, - Accessories, Gus Modern, Touch of Paraffin Wax (just enough to enhance the fragrance delivery), - Bed, White Gardenia, grapefruit | fern | apple wood, 0, Antares Coffee Table, - Candles, Candles, - Desk, - Table, - Sideboard, - Standing, TemaHome, - Innovation Living, $ 2,038.00, Eco-friendly, sustainable resource, 0, - Lounge Chair, Bench, - Side Table, 79“ W x 85”L x 15”FB / 44”HB, Soho Concept, Eastern King / Sable, Queen / Sable, Greenington, Accessories, Your cart, Queen, Coffee Table, 0 Items, Mirrors, LAMPS, Phone : 347-457-5727, DINING, Innovative, Ottoman, Sectional Sofa, - Eilersen, The coordinating Azara bedroom chests each feature an accent drawer faced in solid exotic Tiger bamboo, angled tapered legs, hidden under mount drawer glides and soft closing drawers. Variation in grain and color enrich the natural beauty of bamboo furniture and make each piece unique. Slight differences in bamboo shade colors are signs that the furniture has been crafted from solid bamboo and is considered a benchmark of quality., Lounge Chair, 63”W x 85” L x 15”FB / 44”HB, - Bench, European slat system, no box spring needed, Four Hands, - Ottoman, BRANDS, - Varier, WORKING, - Hanging, Pin It, Hanging, Fatboy, More from this collection, Crafted in 100% solid Moso bamboo, Facebook, Instagram, - Greenington, Login, Table Lamps, 20% harder than Red Oak, Create account |, - Stool, - Sectional Sofa, - Chair, Storage, 820 Manhattan Ave, Brooklyn, NY 11222, ABOUT US, Your cart (0), - Sofa Bed, - Coffee Table, Tweet, King Bed\\xa0 81″W\\xa0\\xa0 88″L\\xa0\\xa0 H-13 1/2″FB/43 1/2″HB, SALE, California King / Sable, California King, - Innovative, Sideboard, Night Table, Hours : Mon-Sat 11am-7pm, Sun 12-5pm, $ 813.00, Expertly crafted in earth friendly solid bamboo, Greenington combines beautiful design, rich colors and a winning sustainability story.\\xa0Available in Queen, Eastern King, and California King sizes, in both caramelized and rich Sable finish., |, Varier, - Office Chair, - Four Hands, Stool, Table, Nuevo, - Table Lamps, Chair, Side Table, Eilersen, POLICY, Pinterest, Sofa Bed, Innovation Living, - Sofa, - Nuevo, Bookcase, Ascent Hi/Low Desk, Enlarge Image, - Fatboy, SLEEPING, Sofa, Office Chair, Antares Console Table, Bed, Queen Bed\\xa0 65.5″W\\xa0 88″L\\xa0\\xa0 H-13 1/2″FB/43 1/2″HB, Desk, - Gus Modern, 76”W x 91 1/4” L x 15”FB / 44”HB, - Night Table, - Bookcase, Antares End Table, - TemaHome, Standing, $ 638.00, - Mirrors, Details, LIVING, - Storage, $ 1,076.00, Eastern King, - Soho Concept, - Accessories, Gus Modern, - Bed, Azara Platform Bed(Sable), 0, - Candles, FRAGRANCE: Fresh, airy and natural; the scent of sea grass, sand, summer sky and the ocean., Candles, - Desk, - Table, Champagne, - Sideboard, - Standing, Default Title, TemaHome, - Innovation Living, 0, - Lounge Chair, Bench, - Side Table, Soho Concept, Greenington, Accessories, Your cart, Coffee Table, 0 Items, Mirrors, LAMPS, Phone : 347-457-5727, DINING, Innovative, Ottoman, Sectional Sofa, - Eilersen, Lounge Chair, Amber, - Bench, Four Hands, - Ottoman, BRANDS, - Varier, WORKING, - Hanging, Sea & Dune, Pin It, Hanging, Fatboy, More from this collection, Facebook, Instagram, - Greenington, Login, Table Lamps, Create account |, - Stool, - Sectional Sofa, - Chair, Storage, 820 Manhattan Ave, Brooklyn, NY 11222, ABOUT US, Your cart (0), - Sofa Bed, - Coffee Table, Tweet, SALE, - Innovative, Sideboard, Night Table, Hours : Mon-Sat 11am-7pm, Sun 12-5pm, |, Varier, - Office Chair, - Four Hands, Stool, Table, Nuevo, $ 70.00, - Table Lamps, Chair, Side Table, Eilersen, POLICY, Pinterest, Sofa Bed, Innovation Living, - Sofa, - Nuevo, Bookcase, Enlarge Image, - Fatboy, SLEEPING, Sofa, Office Chair, Bed, Desk, - Gus Modern, - Night Table, - Bookcase, - TemaHome, Created with natural essential oil-based fragrances, this candle is richly optimized for a 90-hour burn time. The clean-burning soy and paraffin blend is formulated so that the fragrance evenly fills the room. Each hand blown vessel is artisanally crafted and can be re-purposed to live on long after the candle is finished., Standing, - Mirrors, Details, Chamomile Lavender, LIVING, - Storage, Feu de Bois, - Soho Concept, - Accessories, Gus Modern, - Bed'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "furniture_paragraph = ', '.join(pd.read_csv('furniture/results_fulltext_crawler/brooklyncityfurniture.com.csv')['0'].tolist())\n",
    "furniture_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_f = tokenizer(furniture_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1740])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(tokens_f['input_ids']).unsqueeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./furniture-ner.model/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"./furniture-ner.model/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./furniture-ner.model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForTokenClassification.\n",
      "\n",
      "All the weights of DistilBertForTokenClassification were initialized from the model checkpoint at ./furniture-ner.model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1740) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m AutoModelForTokenClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39m./furniture-ner.model/\u001b[39m\u001b[39m'\u001b[39m, num_labels\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(label_list))\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(input_ids\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mtensor(tokens_f[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m),\n\u001b[0;32m      3\u001b[0m                             attention_mask\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mtensor(tokens_f[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n\u001b[0;32m      4\u001b[0m predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(predictions\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39msqueeze(), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m predictions \u001b[39m=\u001b[39m [label_list[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m predictions]\n",
      "File \u001b[1;32md:\\Job\\Assignment Soleadify\\venv\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:990\u001b[0m, in \u001b[0;36mDistilBertForTokenClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[39m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    988\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m--> 990\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[0;32m    991\u001b[0m     input_ids,\n\u001b[0;32m    992\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    993\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    994\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    995\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    996\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    997\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    998\u001b[0m )\n\u001b[0;32m   1000\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1002\u001b[0m sequence_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[1;32md:\\Job\\Assignment Soleadify\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Job\\Assignment Soleadify\\venv\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:578\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    575\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 578\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(input_ids)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    579\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[0;32m    580\u001b[0m     x\u001b[39m=\u001b[39minputs_embeds,\n\u001b[0;32m    581\u001b[0m     attn_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    585\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m    586\u001b[0m )\n",
      "File \u001b[1;32md:\\Job\\Assignment Soleadify\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Job\\Assignment Soleadify\\venv\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:130\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[1;34m(self, input_ids)\u001b[0m\n\u001b[0;32m    127\u001b[0m word_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_embeddings(input_ids)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m    128\u001b[0m position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embeddings(position_ids)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m embeddings \u001b[39m=\u001b[39m word_embeddings \u001b[39m+\u001b[39;49m position_embeddings  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m    131\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(embeddings)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m    132\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embeddings)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (1740) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('./furniture-ner.model/', num_labels=len(label_list))\n",
    "predictions = model.forward(input_ids=torch.tensor(tokens_f['input_ids']).unsqueeze(0),\n",
    "                            attention_mask=torch.tensor(tokens_f['attention_mask']).unsqueeze(0))\n",
    "predictions = torch.argmax(predictions.logits.squeeze(), axis=1)\n",
    "predictions = [label_list[i] for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenizer.batch_decode(tokens_f['input_ids'])\n",
    "pd.DataFrame({'ner': predictions, 'words': words}).to_csv('furniture_ner.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9777d65c1419ccd77fe78e0ccc6ceefa037e4d9fa12bb8f9985a9b89ebc00b86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
